execfile("warmup.krun", globals()) # import the warmup definitions completely


# this file is a minimally hacky way to do startup measurements with all the
# krun goodies, but without changing krun.
#
#
# the way it works is as follows:
#
# before the invocation of the VM, another executable is inserted:
#
# env <some variables> outer_startup_runner_c jruby startup_runner.rb
#
# outer_startup_runner_c is a small C program that prints the current time,
# then execs its arguments. startup_runner.rb will then also print the current
# time and exit. The warmup time is therefore the difference of the two (both
# of these times are stored in the JSON file, the substraction happens in
# post-processing). That way, all the extra commands are not included in the
# measurement, but we still get the krun goodies.
#
# this works by patching two things:
# - the runner file for each VM is patched to point to the startup runner
#   instead (function patch_runner). The startup runners are in
#   startup_runners/
# - the bench_cmdline_adjust method is patched to insert the call to
#   outer_startup_runner_c
#

STARTUP_RUNNER_DIR = os.path.join(DIR, "startup_runners")

ITERATIONS_ALL_VMS = 1

def patch_runner(vm_def):
    from krun.env import EnvChangeAppend
    if vm_def.iterations_runner == "IterationsRunner":
        vm_def.iterations_runner = "startup_runner"
        change = EnvChangeAppend("CLASSPATH", STARTUP_RUNNER_DIR)
        vm_def.add_env_change(change)
    elif vm_def.iterations_runner.endswith("iterations_runner_c"):
        vm_def.iterations_runner = os.path.join(STARTUP_RUNNER_DIR, "startup_runner_c")
    else:
        assert "iterations_runner." in vm_def.iterations_runner
        _, suffix = vm_def.iterations_runner.rsplit(".", 1)
        vm_def.iterations_runner = os.path.join(STARTUP_RUNNER_DIR, "startup_runner." + suffix)
    vm_def.instrument = False
    return vm_def


from krun.platform import BasePlatform, detect_platform

def patch_platform(platform):
    Cls = type(platform)
    assert Cls is not BasePlatform, "must be subclassed"

    def bench_cmdline_adjust(self, args, env_dct):
        # this function inserts the outer runner into the arguments
        prepend_args = BasePlatform.bench_cmdline_adjust(self, [], env_dct)
        # all platform checks are also run with the same mechanism, meaning their
	# output needs to be valid JSON. When running them, this is not the
	# case when using the outer startup runner. Therefore, we don't.
        if "platform_sanity_check" in args[-3]:
            return prepend_args + args
	return prepend_args + ["startup_runners/outer_startup_runner_c"] + args
    old_bench_cmdline_adjust = Cls.__dict__.get('bench_cmdline_adjust')
    assert (old_bench_cmdline_adjust is None or
                old_bench_cmdline_adjust._startup_patch)
    bench_cmdline_adjust._startup_patch = True
    Cls.bench_cmdline_adjust = bench_cmdline_adjust

patch_platform(detect_platform(None, None))

for name, dct in VMS.items():
    dct['vm_def'] = patch_runner(dct['vm_def'])

BENCHMARKS = {
    # this is ignored by the startup runner, but we use one existing one as a
    # dummy
    'binarytrees': 18,
}

# Do not check A/MPERF ratios for the startup experiment.
if "AMPERF_RATIO_BOUNDS" in globals():
    del(BUSY_THRESHOLDS)
    del(AMPERF_RATIO_BOUNDS)
    del(AMPERF_BUSY_THRESHOLD)

N_EXECUTIONS = 200  # Number of fresh processes.
